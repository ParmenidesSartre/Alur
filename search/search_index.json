{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Alur Framework","text":"<p>A production-ready Python framework for building cost-effective data lake pipelines with Apache Spark on AWS.</p> <p> </p>"},{"location":"#overview","title":"Overview","text":"<p>Alur is a production-ready framework for building modern data lake architectures on AWS. Designed with developer experience and operational simplicity in mind, Alur enables data teams to build robust, scalable data pipelines without infrastructure complexity.</p>"},{"location":"#key-features","title":"Key Features","text":"<ul> <li>Declarative Table Definitions - Define tables using Python classes with type-safe schema management</li> <li>Production-Grade Bronze Ingestion - Schema validation, automatic idempotency, multi-source support, and Parquet output</li> <li>File-Level Idempotency - DynamoDB-based state tracking prevents duplicate ingestion and saves costs</li> <li>Multi-Source CSV Ingestion - Ingest from multiple S3 locations in a single pipeline with independent tracking</li> <li>Pipeline Orchestration - Automatic dependency resolution with DAG-based execution</li> <li>Automated Scheduling - Cron-based pipeline scheduling via @schedule decorator with Glue SCHEDULED triggers</li> <li>Data Quality Validation - Built-in quality checks with declarative expectations</li> <li>AWS-Native Deployment - One-command deployment with auto-generated Terraform infrastructure</li> <li>Automatic Partition Registration - Data immediately queryable in Athena after writes</li> <li>Cost Optimization - Serverless architecture with zero idle costs, pay only when pipelines run</li> </ul>"},{"location":"#quick-start","title":"Quick Start","text":""},{"location":"#installation","title":"Installation","text":"<pre><code>pip install alur-framework\n</code></pre>"},{"location":"#initialize-a-project","title":"Initialize a Project","text":"<pre><code>alur init my_datalake\ncd my_datalake\n</code></pre>"},{"location":"#define-a-table","title":"Define a Table","text":"<pre><code>from alur.core import BronzeTable, StringField, IntegerField, TimestampField\n\nclass OrdersBronze(BronzeTable):\n    \"\"\"Raw orders from the source system.\"\"\"\n\n    order_id = StringField(nullable=False)\n    customer_id = StringField(nullable=False)\n    amount = IntegerField(nullable=False)\n    created_at = TimestampField(nullable=False)\n\n    class Meta:\n        partition_by = [\"created_at\"]\n</code></pre>"},{"location":"#create-a-pipeline","title":"Create a Pipeline","text":"<pre><code>from alur import schedule, pipeline\nfrom alur.ingestion import load_to_bronze\nfrom contracts.bronze import OrdersBronze\n\n@schedule(cron=\"0 2 * * ? *\", description=\"Daily order ingestion\")\n@pipeline(sources={}, target=OrdersBronze)\ndef ingest_orders():\n    \"\"\"Daily ingestion of orders with automatic scheduling.\"\"\"\n    spark = get_spark_session()\n\n    return load_to_bronze(\n        spark,\n        source_path=\"s3://landing-zone/orders/*.csv\",\n        source_system=\"sales_db\",\n        target=OrdersBronze,\n        enable_idempotency=True\n    )\n</code></pre>"},{"location":"#deploy-to-aws","title":"Deploy to AWS","text":"<pre><code>alur deploy --env production\n</code></pre>"},{"location":"#target-audience","title":"Target Audience","text":"<ul> <li>Small and mid-size companies building their first data lake</li> <li>Organizations seeking cost-effective alternatives to expensive enterprise platforms</li> <li>Teams without dedicated data infrastructure engineers</li> <li>Companies requiring modern data capabilities without operational complexity</li> </ul>"},{"location":"#core-philosophy","title":"Core Philosophy","text":"<ul> <li>Zero Idle Costs - Built on AWS serverless infrastructure (Glue, S3, Lambda), you only pay when pipelines run</li> <li>Accessible Technology - Enterprise-grade capabilities without enterprise complexity</li> <li>Developer-First - Python-based declarative API that feels familiar to application developers</li> <li>Production-Ready - Built-in best practices for data quality, lineage, and reliability</li> </ul>"},{"location":"#next-steps","title":"Next Steps","text":"<ul> <li>Installation Guide</li> <li>Quick Start Tutorial</li> <li>Core Concepts</li> <li>Bronze Ingestion Guide</li> <li>Scheduling Guide</li> </ul>"},{"location":"#community","title":"Community","text":"<ul> <li>GitHub Repository</li> <li>Issue Tracker</li> <li>Changelog</li> <li>Contributing Guide</li> </ul>"},{"location":"#license","title":"License","text":"<p>This project is licensed under the MIT License. See the LICENSE file for details.</p>"},{"location":"AWS_DEPLOYMENT_GUIDE/","title":"AWS Deployment Guide - How Your Code Runs","text":"<p>This guide explains how your Python code actually runs in AWS Glue.</p>"},{"location":"AWS_DEPLOYMENT_GUIDE/#overview","title":"Overview","text":"<pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  Your Project       \u2502\n\u2502  (Python Code)      \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n           \u2502\n           \u2502 pip install build\n           \u2502 python -m build\n           \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  Wheel Package      \u2502\n\u2502  project-0.1.0.whl  \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n           \u2502\n           \u2502 Upload to S3\n           \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  S3 Artifacts       \u2502\n\u2502  + driver.py        \u2502\n\u2502  + your_project.whl \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n           \u2502\n           \u2502 AWS Glue Job References\n           \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  AWS Glue Job       \u2502\n\u2502  Runs driver.py     \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n           \u2502\n           \u2502 Imports and Executes\n           \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  Your Pipeline      \u2502\n\u2502  Processes Data     \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"AWS_DEPLOYMENT_GUIDE/#step-by-step-deployment","title":"Step-by-Step Deployment","text":""},{"location":"AWS_DEPLOYMENT_GUIDE/#phase-1-package-your-project","title":"Phase 1: Package Your Project","text":"<p>Your project needs to be a proper Python package.</p> <p>Project Structure: <pre><code>my_datalake/\n\u251c\u2500\u2500 setup.py or pyproject.toml\n\u251c\u2500\u2500 config/\n\u251c\u2500\u2500 contracts/\n\u2514\u2500\u2500 pipelines/\n</code></pre></p> <p>Create <code>setup.py</code>: <pre><code>from setuptools import setup, find_packages\n\nsetup(\n    name=\"my_datalake\",\n    version=\"0.1.0\",\n    packages=find_packages(),\n    install_requires=[\n        \"alur-framework&gt;=0.1.0\",\n    ],\n)\n</code></pre></p> <p>Or use <code>pyproject.toml</code>: <pre><code>[build-system]\nrequires = [\"setuptools&gt;=61.0\"]\nbuild-backend = \"setuptools.build_meta\"\n\n[project]\nname = \"my_datalake\"\nversion = \"0.1.0\"\ndependencies = [\n    \"alur-framework&gt;=0.1.0\",\n]\n</code></pre></p> <p>Build the wheel: <pre><code>pip install build\npython -m build\n# Creates: dist/my_datalake-0.1.0-py3-none-any.whl\n</code></pre></p>"},{"location":"AWS_DEPLOYMENT_GUIDE/#phase-2-upload-to-s3","title":"Phase 2: Upload to S3","text":"<p>Upload your wheel and the driver script:</p> <pre><code># Upload your project wheel\naws s3 cp dist/my_datalake-0.1.0-py3-none-any.whl \\\n    s3://alur-artifacts-dev/wheels/\n\n# Upload the Alur driver script\naws s3 cp driver.py s3://alur-artifacts-dev/scripts/driver.py\n</code></pre> <p>The driver.py script is located at: <pre><code>alur-framework/src/alur/templates/aws/driver.py\n</code></pre></p>"},{"location":"AWS_DEPLOYMENT_GUIDE/#phase-3-create-glue-job","title":"Phase 3: Create Glue Job","text":"<p>You can create the Glue job using:</p> <p>A. AWS Console: 1. Go to AWS Glue Console 2. Jobs \u2192 Create Job 3. Configure:    - Script location: <code>s3://alur-artifacts-dev/scripts/driver.py</code>    - Python library path: <code>s3://alur-artifacts-dev/wheels/my_datalake-0.1.0-py3-none-any.whl</code>    - Job parameters:      - <code>--pipeline_name</code>: <code>clean_orders</code>    - IAM Role: Choose role with S3, Glue access    - Glue version: 4.0    - Language: Python 3</p> <p>B. AWS CLI: <pre><code>aws glue create-job \\\n  --name \"alur-clean-orders\" \\\n  --role \"AWSGlueServiceRole\" \\\n  --command '{\n    \"Name\": \"glueetl\",\n    \"ScriptLocation\": \"s3://alur-artifacts-dev/scripts/driver.py\",\n    \"PythonVersion\": \"3\"\n  }' \\\n  --default-arguments '{\n    \"--pipeline_name\": \"clean_orders\",\n    \"--extra-py-files\": \"s3://alur-artifacts-dev/wheels/my_datalake-0.1.0-py3-none-any.whl\"\n  }' \\\n  --glue-version \"4.0\" \\\n  --number-of-workers 2 \\\n  --worker-type \"G.1X\"\n</code></pre></p> <p>C. Terraform (Recommended):</p> <p>Add to your Terraform:</p> <pre><code># glue_jobs.tf\n\nresource \"aws_glue_job\" \"clean_orders\" {\n  name     = \"alur-clean-orders\"\n  role_arn = aws_iam_role.glue_role.arn\n  glue_version = \"4.0\"\n\n  command {\n    name            = \"glueetl\"\n    script_location = \"s3://alur-artifacts-dev/scripts/driver.py\"\n    python_version  = \"3\"\n  }\n\n  default_arguments = {\n    \"--pipeline_name\" = \"clean_orders\"\n    \"--extra-py-files\" = \"s3://alur-artifacts-dev/wheels/my_datalake-0.1.0-py3-none-any.whl\"\n    \"--enable-glue-datacatalog\" = \"\"\n  }\n\n  number_of_workers = 2\n  worker_type       = \"G.1X\"\n\n  tags = {\n    Pipeline = \"clean_orders\"\n    ManagedBy = \"Alur\"\n  }\n}\n</code></pre>"},{"location":"AWS_DEPLOYMENT_GUIDE/#phase-4-run-the-job","title":"Phase 4: Run the Job","text":"<p>Manually trigger: <pre><code>aws glue start-job-run --job-name alur-clean-orders\n</code></pre></p> <p>Check status: <pre><code>aws glue get-job-runs --job-name alur-clean-orders\n</code></pre></p> <p>View logs: - CloudWatch Logs \u2192 <code>/aws-glue/jobs/output</code> - Look for: <code>[Alur Driver]</code> prefix</p>"},{"location":"AWS_DEPLOYMENT_GUIDE/#how-the-driver-script-works","title":"How the Driver Script Works","text":"<p>When Glue runs, here's what happens:</p>"},{"location":"AWS_DEPLOYMENT_GUIDE/#1-driver-script-starts","title":"1. Driver Script Starts","text":"<pre><code># driver.py on S3\nfrom awsglue.utils import getResolvedOptions\nargs = getResolvedOptions(sys.argv, ['pipeline_name'])\n</code></pre>"},{"location":"AWS_DEPLOYMENT_GUIDE/#2-imports-your-code","title":"2. Imports Your Code","text":"<pre><code># Your wheel is in PYTHONPATH via --extra-py-files\nimport pipelines  # This loads your pipelines module\n</code></pre>"},{"location":"AWS_DEPLOYMENT_GUIDE/#3-finds-your-pipeline","title":"3. Finds Your Pipeline","text":"<pre><code>from alur.decorators import PipelineRegistry\npipeline = PipelineRegistry.get(\"clean_orders\")\n</code></pre>"},{"location":"AWS_DEPLOYMENT_GUIDE/#4-runs-your-pipeline","title":"4. Runs Your Pipeline","text":"<pre><code>from alur.engine import AWSAdapter, PipelineRunner\nadapter = AWSAdapter()\nrunner = PipelineRunner(adapter)\nrunner.run_pipeline(\"clean_orders\")\n</code></pre>"},{"location":"AWS_DEPLOYMENT_GUIDE/#5-your-code-executes","title":"5. Your Code Executes","text":"<pre><code># Your pipeline function from pipelines/orders.py\n@pipeline(sources={\"orders\": OrdersBronze}, target=OrdersSilver)\ndef clean_orders(orders):\n    return orders.filter(...)  # Your transformation\n</code></pre>"},{"location":"AWS_DEPLOYMENT_GUIDE/#example-complete-deployment","title":"Example: Complete Deployment","text":"<p>Here's a complete example from start to finish:</p> <pre><code># 1. In your project directory\ncd my_datalake\n\n# 2. Build the wheel\npython -m build\n\n# 3. Upload to S3\naws s3 cp dist/my_datalake-0.1.0-py3-none-any.whl \\\n    s3://alur-artifacts-dev/wheels/\n\n# 4. Upload driver.py (from Alur installation)\naws s3 cp $(python -c \"import alur; print(alur.__path__[0])\")/templates/aws/driver.py \\\n    s3://alur-artifacts-dev/scripts/\n\n# 5. Create Glue job (using CLI)\naws glue create-job \\\n  --name \"alur-clean-orders\" \\\n  --role \"arn:aws:iam::123456789012:role/GlueRole\" \\\n  --command '{\n    \"Name\": \"glueetl\",\n    \"ScriptLocation\": \"s3://alur-artifacts-dev/scripts/driver.py\",\n    \"PythonVersion\": \"3\"\n  }' \\\n  --default-arguments '{\n    \"--pipeline_name\": \"clean_orders\",\n    \"--extra-py-files\": \"s3://alur-artifacts-dev/wheels/my_datalake-0.1.0-py3-none-any.whl\"\n  }' \\\n  --glue-version \"4.0\"\n\n# 6. Run it!\naws glue start-job-run --job-name alur-clean-orders\n\n# 7. Check logs\naws logs tail /aws-glue/jobs/output --follow\n</code></pre>"},{"location":"AWS_DEPLOYMENT_GUIDE/#automated-deployment-script","title":"Automated Deployment Script","text":"<p>Create a <code>deploy.sh</code> script:</p> <pre><code>#!/bin/bash\nset -e\n\nPROJECT_NAME=\"my_datalake\"\nVERSION=$(python -c \"import tomli; print(tomli.load(open('pyproject.toml', 'rb'))['project']['version'])\")\nBUCKET=\"alur-artifacts-dev\"\n\necho \"Deploying $PROJECT_NAME v$VERSION...\"\n\n# Build\necho \"Building wheel...\"\npython -m build\n\n# Upload\necho \"Uploading to S3...\"\naws s3 cp \"dist/${PROJECT_NAME}-${VERSION}-py3-none-any.whl\" \\\n    \"s3://${BUCKET}/wheels/\"\n\n# Upload driver\necho \"Uploading driver...\"\nDRIVER_PATH=$(python -c \"import alur; import os; print(os.path.join(alur.__path__[0], 'templates/aws/driver.py'))\")\naws s3 cp \"$DRIVER_PATH\" \"s3://${BUCKET}/scripts/driver.py\"\n\necho \"\u2713 Deployment complete!\"\necho \"Run with: aws glue start-job-run --job-name alur-&lt;pipeline-name&gt;\"\n</code></pre> <p>Make it executable: <pre><code>chmod +x deploy.sh\n./deploy.sh\n</code></pre></p>"},{"location":"AWS_DEPLOYMENT_GUIDE/#iam-role-requirements","title":"IAM Role Requirements","text":"<p>Your Glue job needs an IAM role with these permissions:</p> <pre><code>{\n  \"Version\": \"2012-10-17\",\n  \"Statement\": [\n    {\n      \"Effect\": \"Allow\",\n      \"Action\": [\n        \"s3:GetObject\",\n        \"s3:PutObject\",\n        \"s3:DeleteObject\",\n        \"s3:ListBucket\"\n      ],\n      \"Resource\": [\n        \"arn:aws:s3:::alur-bronze-dev/*\",\n        \"arn:aws:s3:::alur-silver-dev/*\",\n        \"arn:aws:s3:::alur-gold-dev/*\",\n        \"arn:aws:s3:::alur-artifacts-dev/*\"\n      ]\n    },\n    {\n      \"Effect\": \"Allow\",\n      \"Action\": [\n        \"glue:GetDatabase\",\n        \"glue:GetTable\",\n        \"glue:CreateTable\",\n        \"glue:UpdateTable\",\n        \"glue:GetPartitions\"\n      ],\n      \"Resource\": \"*\"\n    },\n    {\n      \"Effect\": \"Allow\",\n      \"Action\": [\n        \"dynamodb:GetItem\",\n        \"dynamodb:PutItem\",\n        \"dynamodb:UpdateItem\"\n      ],\n      \"Resource\": \"arn:aws:dynamodb:*:*:table/alur-state-dev\"\n    },\n    {\n      \"Effect\": \"Allow\",\n      \"Action\": [\n        \"logs:CreateLogGroup\",\n        \"logs:CreateLogStream\",\n        \"logs:PutLogEvents\"\n      ],\n      \"Resource\": \"arn:aws:logs:*:*:*\"\n    }\n  ]\n}\n</code></pre>"},{"location":"AWS_DEPLOYMENT_GUIDE/#monitoring","title":"Monitoring","text":""},{"location":"AWS_DEPLOYMENT_GUIDE/#cloudwatch-logs","title":"CloudWatch Logs","text":"<p>Glue writes logs to CloudWatch: - <code>/aws-glue/jobs/output</code> - Standard output - <code>/aws-glue/jobs/error</code> - Error logs</p> <p>Look for these log messages: <pre><code>[Alur Driver] Starting job: alur-clean-orders\n[Alur Driver] Pipeline: clean_orders\n[Alur Driver] Found 1 registered pipelines\n[Alur Driver] Executing pipeline: clean_orders\n[Alur] Reading source: orders (ordersbronze)\n[Alur] Executing transformation: clean_orders\n[Alur] Writing to target: orderssilver (mode=merge)\n[Alur] Pipeline 'clean_orders' completed successfully\n[Alur Driver] Pipeline 'clean_orders' completed successfully\n</code></pre></p>"},{"location":"AWS_DEPLOYMENT_GUIDE/#check-job-status","title":"Check Job Status","text":"<pre><code># Get recent runs\naws glue get-job-runs --job-name alur-clean-orders --max-results 5\n\n# Get specific run\naws glue get-job-run --job-name alur-clean-orders --run-id jr_abc123\n</code></pre>"},{"location":"AWS_DEPLOYMENT_GUIDE/#cost-tracking","title":"Cost Tracking","text":"<p>Each Glue job run costs: - Data Processing Units (DPU): $0.44 per DPU-hour - Minimum: 2 DPUs (can configure 1 with G.1X) - Example: 5-minute job with 2 DPUs = $0.44 \u00d7 2 \u00d7 (5/60) = ~$0.07</p> <p>Track costs: <pre><code>aws ce get-cost-and-usage \\\n  --time-period Start=2026-01-01,End=2026-01-31 \\\n  --granularity MONTHLY \\\n  --metrics \"UnblendedCost\" \\\n  --filter file://filter.json\n</code></pre></p>"},{"location":"AWS_DEPLOYMENT_GUIDE/#troubleshooting","title":"Troubleshooting","text":"<p>\"Module not found: pipelines\" - Check <code>--extra-py-files</code> points to correct wheel - Verify wheel was built correctly</p> <p>\"Pipeline 'X' not found\" - Ensure pipeline is imported in your package - Check <code>__init__.py</code> files import the pipeline modules</p> <p>\"Access Denied\" errors - Check IAM role permissions - Verify S3 bucket policies</p> <p>Logs not appearing - Wait 1-2 minutes for logs to appear - Check CloudWatch Logs in correct region</p>"},{"location":"AWS_DEPLOYMENT_GUIDE/#future-alur-deploy-command","title":"Future: alur deploy Command","text":"<p>In Phase 4, this will be automated with:</p> <pre><code>alur deploy --env prod\n</code></pre> <p>Which will: 1. Build your project wheel 2. Upload to S3 3. Update/create Glue jobs 4. Handle versioning</p> <p>But for now, use the manual steps above.</p>"},{"location":"AWS_DEPLOYMENT_GUIDE/#summary","title":"Summary","text":"<p>The flow is: 1. Package your code \u2192 <code>.whl</code> file 2. Upload to S3 \u2192 artifacts bucket 3. Glue job references \u2192 driver.py + your .whl 4. Driver imports your code \u2192 runs pipeline 5. Pipeline processes data \u2192 writes to S3 6. Results available \u2192 in Glue Catalog</p> <p>Key files: - <code>driver.py</code> - Generic runner (provided by Alur) - <code>your_project.whl</code> - Your packaged code - Glue job definition - Points to both</p> <p>This keeps your code separate from the execution environment, making updates easy!</p>"},{"location":"BRONZE_INGESTION/","title":"Bronze Layer Ingestion - Alur Framework","text":""},{"location":"BRONZE_INGESTION/#overview","title":"Overview","text":"<p>The Bronze layer is the raw data landing zone in the medallion architecture. It stores data exactly as received from source systems, with minimal processing and standard metadata for tracking.</p>"},{"location":"BRONZE_INGESTION/#bronze-layer-philosophy","title":"Bronze Layer Philosophy","text":""},{"location":"BRONZE_INGESTION/#what-bronze-is","title":"What Bronze IS","text":"<p>\u2705 Raw data storage - Data as-is from source systems \u2705 Format conversion - CSV/JSON \u2192 Parquet (for efficiency) \u2705 Metadata addition - Track when, where, how data arrived \u2705 Append-only - Immutable audit trail \u2705 Complete history - Keep everything, never delete</p>"},{"location":"BRONZE_INGESTION/#what-bronze-is-not","title":"What Bronze is NOT","text":"<p>\u274c NO transformations - No business logic \u274c NO filtering - Keep bad records for investigation \u274c NO deduplication - Keep duplicates (fix in Silver) \u274c NO data quality fixes - Preserve source truth \u274c NO aggregations - Raw records only</p>"},{"location":"BRONZE_INGESTION/#why-this-matters","title":"Why This Matters","text":"<p>Bronze = Source of Truth for Auditing</p> <ul> <li>Reprocess anytime if business logic changes</li> <li>Investigate data quality issues at the source</li> <li>Regulatory compliance and audit trails</li> <li>Disaster recovery and data lineage</li> </ul>"},{"location":"BRONZE_INGESTION/#production-features","title":"Production Features","text":"<p>Alur's bronze ingestion includes production-grade features for reliability and cost-effectiveness:</p>"},{"location":"BRONZE_INGESTION/#1-schema-validation","title":"1. Schema Validation","text":"<p>Validate incoming data against table contracts before loading:</p> <pre><code>df = load_to_bronze(\n    spark,\n    source_path=\"s3://landing/orders/*.csv\",\n    source_system=\"sales_db\",\n    target=OrdersBronze,  # Validates schema\n    strict_mode=True      # Fail on mismatch\n)\n</code></pre> <p>Benefits: - Catch schema changes early (before bad data enters lake) - Fail-fast prevents expensive downstream errors - Clear error messages for troubleshooting</p>"},{"location":"BRONZE_INGESTION/#2-built-in-logging-metrics","title":"2. Built-in Logging &amp; Metrics","text":"<p>Automatic logging of ingestion metrics:</p> <pre><code>INFO: Starting bronze ingestion from s3://landing/orders/*.csv\nINFO: Source system: sales_db, Target: OrdersBronze\nINFO: Detected format: csv\nINFO: Bronze ingestion completed successfully\nINFO: Rows loaded: 15,000\nINFO: Duration: 12.34s\nINFO: Throughput: 1,215 rows/sec\nINFO: Marked file as processed in state tracker\n</code></pre>"},{"location":"BRONZE_INGESTION/#standard-bronze-metadata","title":"Standard Bronze Metadata","text":"<p>Every Bronze table includes these metadata fields:</p> Field Type Purpose Example <code>_ingested_at</code> Timestamp When data was loaded <code>2024-01-15 14:30:00</code> <code>_source_system</code> String Where data came from <code>sales_db</code>, <code>api</code>, <code>sftp</code> <code>_source_file</code> String Original file name <code>orders_20240115.csv</code> <p>Optional custom metadata: - <code>_batch_id</code> - Batch/job identifier - <code>_environment</code> - Environment (dev/prod) - <code>_data_owner</code> - Team/system responsible - <code>_retention_days</code> - Data retention policy</p>"},{"location":"BRONZE_INGESTION/#quick-start","title":"Quick Start","text":""},{"location":"BRONZE_INGESTION/#1-define-bronze-table-contract","title":"1. Define Bronze Table Contract","text":"<pre><code>from alur.core import BronzeTable, StringField, IntegerField, TimestampField\n\nclass OrdersBronze(BronzeTable):\n    \"\"\"Raw orders from sales system.\"\"\"\n\n    order_id = StringField(nullable=False)\n    customer_id = StringField(nullable=False)\n    amount = IntegerField(nullable=True)\n    created_at = TimestampField(nullable=True)\n\n    # Metadata fields (added automatically by load_to_bronze)\n    _ingested_at = TimestampField(nullable=True)\n    _source_system = StringField(nullable=True)\n    _source_file = StringField(nullable=True)\n\n    class Meta:\n        partition_by = [\"created_at\"]\n        description = \"Raw orders from sales database\"\n</code></pre>"},{"location":"BRONZE_INGESTION/#2-create-production-ingestion-pipeline","title":"2. Create Production Ingestion Pipeline","text":"<pre><code>from alur.decorators import pipeline\nfrom alur.ingestion import load_to_bronze\nfrom alur.engine import get_spark_session\n\n@pipeline(sources={}, target=OrdersBronze)\ndef ingest_orders():\n    \"\"\"\n    Production pattern: Bronze ingestion with schema validation.\n    \"\"\"\n    spark = get_spark_session()\n\n    df = load_to_bronze(\n        spark,\n        source_path=\"s3://landing-zone/orders/*.csv\",\n        source_system=\"sales_db\",\n        target=OrdersBronze,       # Schema validation\n        validate=True,             # Enable validation\n        strict_mode=True,          # Fail on schema mismatch\n    )\n\n    return df\n</code></pre>"},{"location":"BRONZE_INGESTION/#3-deploy-and-run","title":"3. Deploy and Run","text":"<pre><code># Deploy to AWS\nalur deploy --env dev\n\n# Trigger manually:\nalur run ingest_orders\n\n# View logs\nalur logs ingest_orders\n</code></pre>"},{"location":"BRONZE_INGESTION/#api-reference","title":"API Reference","text":""},{"location":"BRONZE_INGESTION/#load_to_bronze","title":"<code>load_to_bronze()</code>","text":"<p>Unified function for loading data into Bronze layer with automatic metadata and safety features.</p> <pre><code>def load_to_bronze(\n    spark: SparkSession,\n    source_path: str,\n    source_system: str,\n    target: Optional[Type] = None,\n    format: Optional[str] = None,\n    options: Optional[Dict[str, str]] = None,\n    schema: Optional[StructType] = None,\n    exclude_metadata: Optional[List[str]] = None,\n    custom_metadata: Optional[Dict[str, Any]] = None,\n    validate: bool = True,\n    strict_mode: bool = True,\n    check_duplicates: bool = True,\n    force_reprocess: bool = False\n) -&gt; DataFrame:\n</code></pre> <p>Parameters:</p> <ul> <li><code>spark</code> - SparkSession instance</li> <li><code>source_path</code> - S3 or local path to files (supports wildcards: <code>*.csv</code>, <code>*.json</code>)</li> <li><code>source_system</code> - Name of source system for metadata</li> <li><code>target</code> - Table contract class for schema validation (e.g., <code>OrdersBronze</code>)</li> <li><code>format</code> - File format ('csv', 'json', 'parquet'). Auto-detected if not provided.</li> <li><code>options</code> - Format-specific reader options (e.g., <code>{\"header\": \"true\"}</code>)</li> <li><code>schema</code> - Optional Spark schema for reading</li> <li><code>exclude_metadata</code> - List of metadata columns to exclude</li> <li><code>custom_metadata</code> - Dictionary of custom metadata to add</li> <li><code>validate</code> - Enable schema validation against target (default: True)</li> <li><code>strict_mode</code> - Fail on extra columns or type mismatches (default: True)</li> <li><code>check_duplicates</code> - NOT IMPLEMENTED - Reserved for future idempotency feature (must keep default value True)</li> <li><code>force_reprocess</code> - NOT IMPLEMENTED - Reserved for future feature (must keep default value False)</li> </ul> <p>Returns: DataFrame with bronze metadata ready for loading</p> <p>Raises: <code>SchemaValidationError</code> if validation fails in strict mode</p> <p>Supported Formats: - CSV (auto-detects from <code>.csv</code> extension) - JSON (auto-detects from <code>.json</code> extension) - Parquet (auto-detects from <code>.parquet</code> extension)</p>"},{"location":"BRONZE_INGESTION/#add_bronze_metadata","title":"<code>add_bronze_metadata()</code>","text":"<p>Manually add bronze metadata to any DataFrame.</p> <pre><code>def add_bronze_metadata(\n    df: DataFrame,\n    source_system: Optional[str] = None,\n    source_file: Optional[str] = None,\n    exclude: Optional[List[str]] = None,\n    custom_metadata: Optional[Dict[str, Any]] = None\n) -&gt; DataFrame:\n</code></pre> <p>Parameters:</p> <ul> <li><code>df</code> - Input DataFrame</li> <li><code>source_system</code> - Name of source system</li> <li><code>source_file</code> - Source file identifier</li> <li><code>exclude</code> - List of metadata columns to exclude (e.g., <code>[\"_source_file\"]</code> for API data)</li> <li><code>custom_metadata</code> - Dictionary of additional metadata fields</li> </ul> <p>Returns: DataFrame with metadata columns added</p>"},{"location":"BRONZE_INGESTION/#validate_schema","title":"<code>validate_schema()</code>","text":"<p>Validate DataFrame schema against a table contract.</p> <pre><code>def validate_schema(\n    df: DataFrame,\n    target: Type,\n    strict_mode: bool = True,\n    exclude_metadata: bool = False\n) -&gt; None:\n</code></pre> <p>Parameters:</p> <ul> <li><code>df</code> - DataFrame to validate</li> <li><code>target</code> - Table contract class (e.g., <code>OrdersBronze</code>)</li> <li><code>strict_mode</code> - If True, fail on extra columns or type mismatches. If False, warn only.</li> <li><code>exclude_metadata</code> - If True, ignore bronze metadata columns in validation</li> </ul> <p>Raises: <code>SchemaValidationError</code> if validation fails in strict mode</p>"},{"location":"BRONZE_INGESTION/#common-patterns","title":"Common Patterns","text":""},{"location":"BRONZE_INGESTION/#pattern-1-csv-file-ingestion-recommended","title":"Pattern 1: CSV File Ingestion (RECOMMENDED)","text":"<pre><code>@pipeline(sources={}, target=OrdersBronze)\ndef ingest_orders():\n    spark = get_spark_session()\n\n    df = load_to_bronze(\n        spark,\n        source_path=\"s3://landing/orders/*.csv\",\n        source_system=\"sales_db\",\n        target=OrdersBronze,\n        validate=True,\n        strict_mode=True\n    )\n\n    return df\n</code></pre> <p>Use when: Ingesting CSV files from S3 with schema validation</p>"},{"location":"BRONZE_INGESTION/#pattern-2-api-data-ingestion","title":"Pattern 2: API Data Ingestion","text":"<pre><code>@pipeline(sources={}, target=EventsBronze)\ndef ingest_api_events():\n    spark = get_spark_session()\n\n    # Read from API response files\n    api_df = spark.read.json(\"s3://api-responses/events/*.json\")\n\n    # Exclude _source_file since this isn't from a file\n    bronze_df = add_bronze_metadata(\n        api_df,\n        source_system=\"events_api\",\n        exclude=[\"_source_file\"],\n        custom_metadata={\n            \"_api_version\": \"v2\",\n            \"_request_id\": \"req_12345\"\n        }\n    )\n\n    return bronze_df\n</code></pre> <p>Use when: Ingesting data from APIs, message queues, or non-file sources</p>"},{"location":"BRONZE_INGESTION/#pattern-3-manual-control-advanced","title":"Pattern 3: Manual Control (Advanced)","text":"<pre><code>@pipeline(sources={}, target=OrdersBronze)\ndef ingest_orders_manual():\n    spark = get_spark_session()\n\n    # Custom read logic with specific options\n    raw_df = spark.read.csv(\n        \"s3://landing/orders/*.csv\",\n        header=True,\n        inferSchema=False,\n        schema=custom_schema,\n        mode=\"FAILFAST\"\n    )\n\n    # Manual metadata addition\n    bronze_df = add_bronze_metadata(\n        raw_df,\n        source_system=\"sales_db\",\n        source_file=\"orders_daily.csv\",\n        custom_metadata={\"_batch_id\": \"batch_001\"}\n    )\n\n    return bronze_df\n</code></pre> <p>Use when: Need fine-grained control over Spark reader options</p>"},{"location":"BRONZE_INGESTION/#cost-optimization-for-smes","title":"Cost Optimization for SMEs","text":""},{"location":"BRONZE_INGESTION/#understanding-bronze-ingestion-costs","title":"Understanding Bronze Ingestion Costs","text":"<p>Bronze ingestion costs come from:</p> <ol> <li>AWS Glue DPU-hours - Compute for running Spark jobs</li> <li>S3 storage - Storing Parquet files</li> <li>S3 requests - Reading source files, writing results</li> </ol>"},{"location":"BRONZE_INGESTION/#cost-saving-features","title":"Cost-Saving Features","text":"<p>1. Parquet format reduces storage costs:</p> <pre><code>CSV: 1GB of data = $0.023/month\nParquet: Same data compressed to 200MB = $0.005/month (78% savings)\n</code></pre>"},{"location":"BRONZE_INGESTION/#example-monthly-cost-for-1m-rowsday","title":"Example: Monthly Cost for 1M Rows/Day","text":"Component Without Parquet With Parquet Savings Storage (CSV) $7 $1.50 (compression) $5.50 Monthly Total $7 $1.50 $5.50 (79% savings) <p>Note: Additional cost savings can be achieved by implementing your own idempotency logic to prevent duplicate processing.</p>"},{"location":"BRONZE_INGESTION/#troubleshooting","title":"Troubleshooting","text":""},{"location":"BRONZE_INGESTION/#schema-validation-failed","title":"Schema Validation Failed","text":"<p>Error: <pre><code>SchemaValidationError: Missing required column 'customer_id'\n</code></pre></p> <p>Solution: 1. Check if source file format changed 2. Verify column names match exactly (case-sensitive) 3. For development, use <code>strict_mode=False</code> to see warnings instead of errors</p>"},{"location":"BRONZE_INGESTION/#no-data-loaded","title":"No Data Loaded","text":"<p>Issue: DataFrame is empty after <code>load_to_bronze()</code></p> <p>Solutions: 1. Verify S3 path is correct 2. Check if wildcard pattern matches files 3. Ensure AWS credentials have S3 read permissions 4. Check CloudWatch logs for errors</p>"},{"location":"BRONZE_INGESTION/#best-practices","title":"Best Practices","text":""},{"location":"BRONZE_INGESTION/#1-enable-schema-validation-in-production","title":"1. Enable Schema Validation in Production","text":"<pre><code># Production (strict)\ndf = load_to_bronze(..., target=OrdersBronze, strict_mode=True)\n\n# Development (relaxed)\ndf = load_to_bronze(..., target=OrdersBronze, strict_mode=False)\n</code></pre>"},{"location":"BRONZE_INGESTION/#2-use-descriptive-source-system-names","title":"2. Use Descriptive Source System Names","text":"<pre><code># Good\nsource_system=\"salesforce_crm\"\nsource_system=\"stripe_payments\"\nsource_system=\"internal_hr_db\"\n\n# Bad\nsource_system=\"db1\"\nsource_system=\"api\"\nsource_system=\"data\"\n</code></pre>"},{"location":"BRONZE_INGESTION/#current-limitations","title":"Current Limitations","text":"<p>The following features are not yet implemented in the Spark-based <code>load_to_bronze()</code> function:</p>"},{"location":"BRONZE_INGESTION/#1-idempotency-duplicate-detection","title":"1. Idempotency / Duplicate Detection","text":"<p>Status: Not implemented</p> <p>Impact: Files will be reprocessed every time the pipeline runs, potentially causing: - Duplicate records in bronze tables - Increased storage costs - Wasted compute resources</p> <p>Workarounds: - Use the batch ingestion module (<code>alur.batch_ingestion.ingest_csv_sources_to_bronze()</code>) which has better support for batch processing - Implement your own deduplication logic (e.g., check DynamoDB before processing) - Use unique file paths and manual file management - Add deduplication in Silver layer transformations</p> <p>Example of manual deduplication: <pre><code>import boto3\nfrom datetime import datetime\n\ndynamodb = boto3.resource('dynamodb')\ntable = dynamodb.Table('alur-ingestion-state')\n\ndef should_process_file(file_path: str) -&gt; bool:\n    \"\"\"Check if file was already processed.\"\"\"\n    try:\n        response = table.get_item(Key={'file_path': file_path})\n        return 'Item' not in response\n    except:\n        return True\n\ndef mark_processed(file_path: str):\n    \"\"\"Mark file as processed.\"\"\"\n    table.put_item(Item={\n        'file_path': file_path,\n        'processed_at': datetime.utcnow().isoformat()\n    })\n</code></pre></p>"},{"location":"BRONZE_INGESTION/#2-scheduled-execution","title":"2. Scheduled Execution","text":"<p>Status: \u2705 Available (via @schedule decorator)</p> <p>Feature: Automatic pipeline triggering via AWS Glue SCHEDULED triggers</p> <p>Usage: <pre><code>from alur import schedule, pipeline\n\n@schedule(cron=\"0 2 * * ? *\", description=\"Daily ingestion\")\n@pipeline(sources={}, target=OrdersBronze)\ndef ingest_orders():\n    return load_to_bronze(...)\n</code></pre></p> <p>See SCHEDULING.md for complete documentation. - Use AWS Glue workflow triggers</p>"},{"location":"BRONZE_INGESTION/#3-sample-based-validation-only","title":"3. Sample-Based Validation Only","text":"<p>Status: By design (in batch ingestion)</p> <p>Limitation: CSV validation only checks first 50 rows by default</p> <p>Impact: Schema errors in rows beyond the sample may go undetected</p> <p>Workarounds: - Increase <code>sample_rows</code> parameter (e.g., <code>sample_rows=1000</code>) - Set <code>sample_rows=None</code> for full file validation (slower, more expensive) - Rely on Spark schema enforcement to catch issues during read - Add data quality checks in Silver layer</p>"},{"location":"BRONZE_INGESTION/#4-csv-only-support-for-spark-based-ingestion","title":"4. CSV-Only Support (for Spark-based ingestion)","text":"<p>Status: By design (AWS-only, batch-focused framework)</p> <p>Limitation: Only CSV files from S3 are supported</p> <p>Workarounds: - Use Spark directly for other formats - Convert files to CSV before ingestion - Use batch ingestion for CSV, custom code for other formats</p>"},{"location":"BRONZE_INGESTION/#next-steps","title":"Next Steps","text":"<p>After loading data into Bronze:</p> <ol> <li>Create Silver pipelines - Transform and clean data</li> <li>Add data quality checks - Use <code>@expect</code> decorator</li> <li>Set up monitoring - Check CloudWatch logs</li> <li>Review costs - Monitor AWS Glue DPU usage</li> </ol> <p>See Data Quality for validation patterns.</p>"},{"location":"DATA_QUALITY/","title":"Data Quality Checks - Alur Framework","text":""},{"location":"DATA_QUALITY/#overview","title":"Overview","text":"<p>Alur provides built-in data quality validation that runs automatically after each pipeline execution. Catch data issues early and ensure data reliability across your entire lakehouse.</p>"},{"location":"DATA_QUALITY/#features","title":"Features","text":"<ul> <li>\u2705 Automatic execution - Checks run after pipeline completes</li> <li>\u2705 Multiple severity levels - ERROR (fails pipeline) or WARN (logs only)</li> <li>\u2705 Built-in checks - Common validations ready to use</li> <li>\u2705 Custom checks - Write your own validation logic</li> <li>\u2705 Zero overhead - Only runs on result DataFrame</li> <li>\u2705 Clear reporting - Detailed pass/fail messages</li> </ul>"},{"location":"DATA_QUALITY/#quick-start","title":"Quick Start","text":""},{"location":"DATA_QUALITY/#basic-usage","title":"Basic Usage","text":"<pre><code>from alur.decorators import pipeline\nfrom alur.quality import expect, not_empty, no_nulls_in_column\n\n@expect(\n    name=\"data_not_empty\",\n    check_fn=not_empty,\n    description=\"Ensure we have data\"\n)\n@expect(\n    name=\"no_null_ids\",\n    check_fn=no_nulls_in_column(\"order_id\"),\n    description=\"All orders must have an ID\"\n)\n@pipeline(\n    sources={\"orders\": OrdersBronze},\n    target=OrdersSilver\n)\ndef clean_orders(orders):\n    return orders.filter(F.col(\"status\") == \"valid\")\n</code></pre>"},{"location":"DATA_QUALITY/#what-happens","title":"What Happens","text":"<pre><code>[Alur] Executing transformation: clean_orders\n[Alur] Writing to target: orderssilver (mode=overwrite)\n[Alur] Running 2 quality check(s)...\n[Alur]   \u2713 data_not_empty: DataFrame has 1234 rows\n[Alur]   \u2713 no_null_ids: Column 'order_id' has 0 null values\n[Alur] Pipeline 'clean_orders' completed successfully\n</code></pre>"},{"location":"DATA_QUALITY/#built-in-quality-checks","title":"Built-in Quality Checks","text":""},{"location":"DATA_QUALITY/#1-row-count-checks","title":"1. Row Count Checks","text":"<pre><code>from alur.quality import not_empty, min_row_count, max_row_count\n\n# Check DataFrame is not empty\n@expect(name=\"has_data\", check_fn=not_empty)\n\n# Minimum row count\n@expect(name=\"min_rows\", check_fn=min_row_count(1000))\n\n# Maximum row count\n@expect(name=\"max_rows\", check_fn=max_row_count(1000000))\n</code></pre>"},{"location":"DATA_QUALITY/#2-null-checks","title":"2. Null Checks","text":"<pre><code>from alur.quality import no_nulls_in_column\n\n# No nulls in critical columns\n@expect(name=\"no_null_ids\", check_fn=no_nulls_in_column(\"order_id\"))\n@expect(name=\"no_null_amounts\", check_fn=no_nulls_in_column(\"amount\"))\n</code></pre>"},{"location":"DATA_QUALITY/#3-duplicate-checks","title":"3. Duplicate Checks","text":"<pre><code>from alur.quality import no_duplicates_in_column\n\n# Primary key uniqueness\n@expect(name=\"unique_ids\", check_fn=no_duplicates_in_column(\"order_id\"))\n</code></pre>"},{"location":"DATA_QUALITY/#4-schema-validation","title":"4. Schema Validation","text":"<pre><code>from alur.quality import schema_has_columns\n\n# Required columns present\n@expect(\n    name=\"required_columns\",\n    check_fn=schema_has_columns([\"order_id\", \"customer_id\", \"amount\"])\n)\n</code></pre>"},{"location":"DATA_QUALITY/#5-value-range-checks","title":"5. Value Range Checks","text":"<pre><code>from alur.quality import column_values_in_range\n\n# Values within expected range\n@expect(\n    name=\"valid_amounts\",\n    check_fn=column_values_in_range(\"amount\", min_val=0, max_val=100000)\n)\n\n@expect(\n    name=\"valid_quantities\",\n    check_fn=column_values_in_range(\"quantity\", min_val=1, max_val=1000)\n)\n</code></pre>"},{"location":"DATA_QUALITY/#6-freshness-checks","title":"6. Freshness Checks","text":"<pre><code>from alur.quality import freshness_check\n\n# Data recency\n@expect(\n    name=\"data_is_fresh\",\n    check_fn=freshness_check(\"created_at\", max_age_hours=24)\n)\n</code></pre>"},{"location":"DATA_QUALITY/#custom-quality-checks","title":"Custom Quality Checks","text":""},{"location":"DATA_QUALITY/#simple-custom-check","title":"Simple Custom Check","text":"<pre><code>def check_no_future_dates(df):\n    \"\"\"Ensure no orders have future dates.\"\"\"\n    from pyspark.sql.functions import col, current_timestamp\n\n    future_count = df.filter(col(\"created_at\") &gt; current_timestamp()).count()\n\n    if future_count == 0:\n        return (True, \"No future-dated orders\")\n    else:\n        return (False, f\"Found {future_count} orders with future dates\")\n\n@expect(name=\"no_future_orders\", check_fn=check_no_future_dates)\n@pipeline(...)\ndef my_pipeline(data):\n    return data\n</code></pre>"},{"location":"DATA_QUALITY/#lambda-checks","title":"Lambda Checks","text":"<pre><code># Quick inline check\n@expect(\n    name=\"weekend_orders\",\n    check_fn=lambda df: (\n        df.filter(F.dayofweek(\"created_at\").isin([1, 7])).count() &gt; 0,\n        \"Has weekend orders\"\n    )\n)\n</code></pre>"},{"location":"DATA_QUALITY/#complex-custom-check","title":"Complex Custom Check","text":"<pre><code>def check_revenue_growth(df):\n    \"\"\"\n    Ensure daily revenue is growing.\n    Returns (bool, str) tuple.\n    \"\"\"\n    from pyspark.sql import Window\n    from pyspark.sql.functions import col, lag\n\n    # Calculate day-over-day growth\n    window = Window.orderBy(\"report_date\")\n\n    with_previous = df.withColumn(\n        \"prev_revenue\",\n        lag(\"total_revenue\").over(window)\n    )\n\n    declining_days = with_previous.filter(\n        col(\"total_revenue\") &lt; col(\"prev_revenue\")\n    ).count()\n\n    passed = declining_days == 0\n\n    if passed:\n        return (True, \"Revenue growing consistently\")\n    else:\n        return (False, f\"Revenue declined on {declining_days} days\")\n\n@expect(name=\"revenue_growth\", check_fn=check_revenue_growth)\n@pipeline(...)\ndef sales_analysis(data):\n    return data\n</code></pre>"},{"location":"DATA_QUALITY/#severity-levels","title":"Severity Levels","text":""},{"location":"DATA_QUALITY/#error-default","title":"ERROR (Default)","text":"<p>Pipeline fails if check fails.</p> <pre><code>from alur.quality import Severity\n\n@expect(\n    name=\"critical_check\",\n    check_fn=min_row_count(100),\n    severity=Severity.ERROR  # Pipeline fails\n)\n</code></pre> <p>Output when fails: <pre><code>[Alur]   \u2717 critical_check: Row count 50 below minimum 100\n[Alur] Quality checks: 1 failure(s)\n[ERROR] Data quality checks failed for pipeline 'my_pipeline':\n  - critical_check: Row count 50 below minimum 100\n</code></pre></p>"},{"location":"DATA_QUALITY/#warn","title":"WARN","text":"<p>Pipeline continues but logs warning.</p> <pre><code>@expect(\n    name=\"preferred_check\",\n    check_fn=min_row_count(1000),\n    severity=Severity.WARN  # Only warns\n)\n</code></pre> <p>Output when fails: <pre><code>[Alur]   \u26a0  preferred_check: Row count 500 below minimum 1000\n[Alur] Quality checks: 1 warning(s)\n[Alur] Pipeline 'my_pipeline' completed successfully\n</code></pre></p>"},{"location":"DATA_QUALITY/#real-world-examples","title":"Real-World Examples","text":""},{"location":"DATA_QUALITY/#example-1-e-commerce-orders","title":"Example 1: E-commerce Orders","text":"<pre><code>from alur.quality import (\n    expect, Severity,\n    not_empty, min_row_count,\n    no_nulls_in_column, no_duplicates_in_column,\n    column_values_in_range, freshness_check\n)\n\n@expect(\n    name=\"orders_not_empty\",\n    check_fn=not_empty,\n    description=\"Must have order data\"\n)\n@expect(\n    name=\"min_daily_orders\",\n    check_fn=min_row_count(100),\n    severity=Severity.ERROR,\n    description=\"Expect at least 100 orders per day\"\n)\n@expect(\n    name=\"unique_order_ids\",\n    check_fn=no_duplicates_in_column(\"order_id\"),\n    description=\"Order IDs must be unique\"\n)\n@expect(\n    name=\"no_null_customers\",\n    check_fn=no_nulls_in_column(\"customer_id\"),\n    description=\"All orders need customer IDs\"\n)\n@expect(\n    name=\"valid_amounts\",\n    check_fn=column_values_in_range(\"amount\", 0, 1000000),\n    description=\"Amounts between $0 and $10,000\"\n)\n@expect(\n    name=\"fresh_data\",\n    check_fn=freshness_check(\"created_at\", max_age_hours=2),\n    description=\"Data should be &lt;2 hours old\"\n)\n@pipeline(sources={\"orders\": OrdersBronze}, target=OrdersSilver)\ndef process_orders(orders):\n    return orders.filter(F.col(\"status\") == \"completed\")\n</code></pre>"},{"location":"DATA_QUALITY/#example-2-financial-reconciliation","title":"Example 2: Financial Reconciliation","text":"<pre><code>def check_debit_credit_balance(df):\n    \"\"\"Ensure debits equal credits.\"\"\"\n    from pyspark.sql.functions import sum as spark_sum, col\n\n    totals = df.agg(\n        spark_sum(col(\"debit\")).alias(\"total_debit\"),\n        spark_sum(col(\"credit\")).alias(\"total_credit\")\n    ).collect()[0]\n\n    debit = totals[\"total_debit\"]\n    credit = totals[\"total_credit\"]\n\n    balanced = abs(debit - credit) &lt; 0.01  # Allow for rounding\n\n    return (\n        balanced,\n        f\"Debits ({debit}) and credits ({credit}) {'balanced' if balanced else 'UNBALANCED'}\"\n    )\n\n@expect(\n    name=\"books_balanced\",\n    check_fn=check_debit_credit_balance,\n    severity=Severity.ERROR,\n    description=\"Accounting entries must balance\"\n)\n@pipeline(...)\ndef reconcile_accounts(transactions):\n    return transactions\n</code></pre>"},{"location":"DATA_QUALITY/#example-3-data-warehouse-etl","title":"Example 3: Data Warehouse ETL","text":"<pre><code>@expect(\n    name=\"no_duplicates\",\n    check_fn=no_duplicates_in_column(\"transaction_id\"),\n    severity=Severity.ERROR\n)\n@expect(\n    name=\"all_required_columns\",\n    check_fn=schema_has_columns([\n        \"transaction_id\", \"customer_id\", \"product_id\",\n        \"amount\", \"timestamp\"\n    ]),\n    severity=Severity.ERROR\n)\n@expect(\n    name=\"reasonable_transaction_amounts\",\n    check_fn=column_values_in_range(\"amount\", -10000, 10000),\n    severity=Severity.WARN\n)\n@expect(\n    name=\"has_weekend_data\",\n    check_fn=lambda df: (\n        df.filter(F.dayofweek(\"timestamp\").isin([1, 7])).count() &gt; 0,\n        \"Should have weekend transactions\"\n    ),\n    severity=Severity.WARN\n)\n@pipeline(sources={\"txns\": TransactionsBronze}, target=TransactionsSilver)\ndef clean_transactions(txns):\n    \"\"\"Clean transactions with comprehensive quality checks.\"\"\"\n    return txns.filter(F.col(\"status\") == \"completed\")\n</code></pre>"},{"location":"DATA_QUALITY/#check-execution-flow","title":"Check Execution Flow","text":"<pre><code>1. Pipeline executes transformation\n2. Result DataFrame is written to target\n3. Quality checks run on result DataFrame\n4. For each check:\n   - Run check function\n   - Log result (\u2713 pass, \u2717 fail, \u26a0 warn)\n5. If any ERROR-level checks fail:\n   - Raise RuntimeError with details\n   - Pipeline fails\n6. If only WARN-level checks fail:\n   - Log warnings\n   - Pipeline succeeds\n7. Pipeline completes\n</code></pre>"},{"location":"DATA_QUALITY/#disabling-checks","title":"Disabling Checks","text":""},{"location":"DATA_QUALITY/#temporarily-disable-a-check","title":"Temporarily Disable a Check","text":"<pre><code>@expect(\n    name=\"optional_check\",\n    check_fn=min_row_count(1000),\n    enabled=False  # Disabled\n)\n</code></pre>"},{"location":"DATA_QUALITY/#environment-specific-checks","title":"Environment-Specific Checks","text":"<pre><code>import os\n\n@expect(\n    name=\"prod_only_check\",\n    check_fn=min_row_count(10000),\n    enabled=(os.getenv(\"ENV\") == \"prod\")  # Only in production\n)\n</code></pre>"},{"location":"DATA_QUALITY/#viewing-quality-checks","title":"Viewing Quality Checks","text":""},{"location":"DATA_QUALITY/#list-all-checks","title":"List All Checks","text":"<pre><code>$ alur list --verbose\n\n\u2699\ufe0f  PIPELINES (1 total)\n\n  \u2022 clean_orders: [orders] \u2192 orderssilver\n      Sources: orders\n      Target: orderssilver\n      Profile: medium\n      Quality Checks: 3\n        - data_not_empty (ERROR)\n        - min_1000_orders (ERROR)\n        - no_null_ids (ERROR)\n</code></pre>"},{"location":"DATA_QUALITY/#programmatic-access","title":"Programmatic Access","text":"<pre><code>from alur.quality import QualityRegistry\n\n# Get checks for a pipeline\nchecks = QualityRegistry.get_checks(\"clean_orders\")\n\nfor check in checks:\n    print(f\"Check: {check.name}\")\n    print(f\"  Severity: {check.severity.value}\")\n    print(f\"  Enabled: {check.enabled}\")\n    print(f\"  Description: {check.description}\")\n</code></pre>"},{"location":"DATA_QUALITY/#best-practices","title":"Best Practices","text":""},{"location":"DATA_QUALITY/#1-start-with-critical-checks","title":"1. Start with Critical Checks","text":"<pre><code># Start with must-have validations\n@expect(name=\"not_empty\", check_fn=not_empty, severity=Severity.ERROR)\n@expect(name=\"no_null_pks\", check_fn=no_nulls_in_column(\"id\"), severity=Severity.ERROR)\n</code></pre>"},{"location":"DATA_QUALITY/#2-add-warnings-for-nice-to-haves","title":"2. Add Warnings for Nice-to-Haves","text":"<pre><code># Add softer checks as warnings\n@expect(name=\"preferred_volume\", check_fn=min_row_count(1000), severity=Severity.WARN)\n</code></pre>"},{"location":"DATA_QUALITY/#3-keep-checks-fast","title":"3. Keep Checks Fast","text":"<pre><code># Good: Uses count() - fast\ncheck_fn=min_row_count(100)\n\n# Bad: Collects all data - slow\ncheck_fn=lambda df: (len(df.collect()) &gt; 100, \"...\")\n</code></pre>"},{"location":"DATA_QUALITY/#4-use-descriptive-names","title":"4. Use Descriptive Names","text":"<pre><code># Good\n@expect(name=\"no_future_order_dates\", ...)\n\n# Bad\n@expect(name=\"check1\", ...)\n</code></pre>"},{"location":"DATA_QUALITY/#5-layer-your-validations","title":"5. Layer Your Validations","text":"<pre><code># Bronze: Basic structure\n@expect(name=\"has_data\", check_fn=not_empty)\n@expect(name=\"has_required_columns\", check_fn=schema_has_columns([...]))\n\n# Silver: Business rules\n@expect(name=\"no_null_ids\", check_fn=no_nulls_in_column(\"id\"))\n@expect(name=\"valid_amounts\", check_fn=column_values_in_range(\"amount\", 0, 100000))\n\n# Gold: Analytics quality\n@expect(name=\"reasonable_aggregates\", check_fn=...)\n@expect(name=\"no_negative_totals\", check_fn=...)\n</code></pre>"},{"location":"DATA_QUALITY/#advanced-patterns","title":"Advanced Patterns","text":""},{"location":"DATA_QUALITY/#check-with-external-state","title":"Check with External State","text":"<pre><code>def check_against_previous_run(df):\n    \"\"\"Compare to previous run metrics.\"\"\"\n    current_count = df.count()\n\n    # TODO: Load previous count from DynamoDB/S3\n    # previous_count = load_from_state_store()\n\n    # For now, return pass\n    return (True, f\"Current count: {current_count}\")\n</code></pre>"},{"location":"DATA_QUALITY/#multi-column-checks","title":"Multi-Column Checks","text":"<pre><code>def check_referential_integrity(df):\n    \"\"\"Ensure customer_id exists in customers table.\"\"\"\n    # This would require access to other tables\n    # Could be implemented by joining with customer dimension\n    return (True, \"Referential integrity maintained\")\n</code></pre>"},{"location":"DATA_QUALITY/#statistical-checks","title":"Statistical Checks","text":"<pre><code>def check_distribution(df):\n    \"\"\"Ensure data distribution is reasonable.\"\"\"\n    from pyspark.sql.functions import stddev, mean\n\n    stats = df.agg(\n        mean(\"amount\").alias(\"mean\"),\n        stddev(\"amount\").alias(\"stddev\")\n    ).collect()[0]\n\n    # Check if stddev is too high (possible data quality issue)\n    coefficient_of_variation = stats[\"stddev\"] / stats[\"mean\"]\n\n    passed = coefficient_of_variation &lt; 2.0\n\n    return (\n        passed,\n        f\"CV: {coefficient_of_variation:.2f} ({'normal' if passed else 'high variance'})\"\n    )\n</code></pre>"},{"location":"DATA_QUALITY/#integration-with-contract-validation","title":"Integration with Contract Validation","text":"<p>Quality checks work seamlessly with contract field constraints:</p> <pre><code>from alur.core import SilverTable, StringField, IntegerField\n\nclass OrdersSilver(SilverTable):\n    \"\"\"Contract defines schema constraints.\"\"\"\n\n    order_id = StringField(nullable=False)  # Schema-level constraint\n    amount = IntegerField(nullable=False)   # Schema-level constraint\n\n# Quality checks add runtime validation\n@expect(\n    name=\"no_null_order_ids\",  # Runtime validation\n    check_fn=no_nulls_in_column(\"order_id\")\n)\n@expect(\n    name=\"positive_amounts\",   # Business rule validation\n    check_fn=column_values_in_range(\"amount\", 0, 1000000)\n)\n@pipeline(sources={...}, target=OrdersSilver)\ndef process_orders(data):\n    return data\n</code></pre> <p>Layer your validations: - Schema (Contracts): Field types, nullability - Quality Checks: Business rules, data ranges, freshness - Custom Logic: Complex validations in pipeline code</p>"},{"location":"DATA_QUALITY/#summary","title":"Summary","text":"<p>Data quality checks in Alur provide:</p> <p>\u2705 Automatic validation after every pipeline run \u2705 Built-in checks for common scenarios \u2705 Custom checks for business-specific rules \u2705 Flexible severity (ERROR vs WARN) \u2705 Clear reporting with detailed messages \u2705 Zero config - just add decorators</p> <p>Start with basic checks, add custom validations as needed, and ensure data quality across your entire lakehouse!</p>"},{"location":"DEPLOY/","title":"Alur One-Command Deployment","text":"<p>Deploy your Alur data lake project to AWS with a single command.</p>"},{"location":"DEPLOY/#overview","title":"Overview","text":"<p>The <code>alur deploy</code> command handles the entire deployment workflow:</p> <ol> <li>Load Configuration - Reads your project settings</li> <li>Build Python Wheel - Packages your pipelines, contracts, and config</li> <li>Generate Terraform - Creates infrastructure-as-code files</li> <li>Apply Terraform - Provisions AWS resources (S3, Glue, DynamoDB)</li> <li>Upload to S3 - Deploys your code and driver script</li> <li>Summary - Shows deployment details and next steps</li> </ol>"},{"location":"DEPLOY/#quick-start","title":"Quick Start","text":"<pre><code># Full deployment (dev environment)\ncd my_data_lake\nalur deploy --env dev\n\n# Production deployment with auto-approval\nalur deploy --env prod --auto-approve\n\n# Just upload code (skip infrastructure)\nalur deploy --skip-terraform\n\n# Quick redeploy (reuse existing wheel)\nalur deploy --skip-build --skip-terraform\n</code></pre>"},{"location":"DEPLOY/#prerequisites","title":"Prerequisites","text":""},{"location":"DEPLOY/#required","title":"Required","text":"<ul> <li>Python 3.8+</li> <li>AWS CLI configured with credentials</li> <li>Your Alur project initialized with <code>alur init</code></li> </ul>"},{"location":"DEPLOY/#optional","title":"Optional","text":"<ul> <li>Terraform (if not using --skip-terraform)</li> <li><code>build</code> module: <code>pip install build</code></li> </ul>"},{"location":"DEPLOY/#command-options","title":"Command Options","text":"Option Description <code>--env</code> Environment name (dev, staging, prod). Default: dev <code>--skip-build</code> Skip building the wheel, use existing one in dist/ <code>--skip-terraform</code> Skip Terraform generation and apply <code>--auto-approve</code> Auto-approve Terraform changes (no confirmation)"},{"location":"DEPLOY/#what-gets-deployed","title":"What Gets Deployed","text":""},{"location":"DEPLOY/#your-code-wheel-package","title":"Your Code (Wheel Package)","text":"<ul> <li><code>contracts/</code> - Your table definitions</li> <li><code>pipelines/</code> - Your pipeline functions</li> <li><code>config/</code> - Your configuration settings</li> </ul> <p>Location: <code>s3://alur-artifacts-{env}/wheels/{project}-{version}.whl</code></p>"},{"location":"DEPLOY/#driver-script","title":"Driver Script","text":"<p>Generic Glue execution script that loads your wheel and runs pipelines.</p> <p>Location: <code>s3://alur-artifacts-{env}/scripts/driver.py</code></p>"},{"location":"DEPLOY/#infrastructure-if-not-skipped","title":"Infrastructure (if not skipped)","text":"<ul> <li>S3 buckets: bronze, silver, gold, artifacts</li> <li>IAM roles: Glue execution role</li> <li>DynamoDB table: watermarks and state</li> <li>Glue catalog: databases for each layer</li> </ul>"},{"location":"DEPLOY/#example-workflow","title":"Example Workflow","text":""},{"location":"DEPLOY/#first-deployment","title":"First Deployment","text":"<pre><code># 1. Create your project\nalur init my_datalake\ncd my_datalake\n\n# 2. Edit your contracts and pipelines\n# ... edit contracts/bronze.py, contracts/silver.py\n# ... edit pipelines/orders.py\n\n# 3. Configure AWS settings\n# Edit config/settings.py with your bucket names\n\n# 4. Deploy everything\nalur deploy --env dev\n</code></pre>"},{"location":"DEPLOY/#subsequent-updates","title":"Subsequent Updates","text":"<pre><code># Made code changes? Redeploy just the code\nalur deploy --skip-terraform\n\n# Test build without deploying\npython -m build\n</code></pre>"},{"location":"DEPLOY/#output-example","title":"Output Example","text":"<pre><code>============================================================\nALUR DEPLOYMENT - Environment: dev\n============================================================\n\n[1/6] Loading configuration...\n  Region: us-east-1\n  Artifacts bucket: alur-artifacts-dev\n\n[2/6] Building Python wheel...\n  [OK] Build complete\n  Wheel: dist/my_datalake-0.1.0-py3-none-any.whl\n\n[3/6] Generating Terraform files...\n  [OK] Terraform files generated\n\n[4/6] Applying Terraform...\n  Running: terraform init...\n  Running: terraform apply...\n  [OK] Infrastructure deployed\n\n[5/6] Uploading to S3...\n  Uploading: dist/my_datalake-0.1.0-py3-none-any.whl\n  To: s3://alur-artifacts-dev/wheels/my_datalake-0.1.0-py3-none-any.whl\n  [OK] Wheel uploaded\n  Uploading: driver.py\n  [OK] Driver script uploaded\n\n[6/6] Deployment Summary\n  ========================================================\n  Environment: dev\n  Wheel: my_datalake-0.1.0-py3-none-any.whl\n  S3 Location: s3://alur-artifacts-dev/wheels/my_datalake-0.1.0-py3-none-any.whl\n  Region: us-east-1\n  ========================================================\n\n[OK] Deployment complete!\n\nNext steps:\n  1. Create Glue jobs pointing to:\n     Script: s3://alur-artifacts-dev/scripts/driver.py\n     Library: s3://alur-artifacts-dev/wheels/my_datalake-0.1.0-py3-none-any.whl\n  2. Or use AWS Console to create jobs manually\n  3. Run with: aws glue start-job-run --job-name &lt;job-name&gt;\n</code></pre>"},{"location":"DEPLOY/#how-glue-jobs-work","title":"How Glue Jobs Work","text":"<p>Once deployed, create a Glue job with:</p> <p>Script location: <code>s3://alur-artifacts-{env}/scripts/driver.py</code> Python library path: <code>s3://alur-artifacts-{env}/wheels/{project}.whl</code> Job parameters: - <code>--pipeline_name</code>: Name of the pipeline to run (e.g., <code>clean_orders</code>)</p> <p>The driver script: 1. Loads your wheel package 2. Finds the specified pipeline in the registry 3. Executes it with the AWS adapter 4. Writes results to S3 (silver/gold layers)</p>"},{"location":"DEPLOY/#troubleshooting","title":"Troubleshooting","text":""},{"location":"DEPLOY/#build-fails","title":"Build fails","text":"<p><pre><code>[ERROR] 'build' module not found. Install: pip install build\n</code></pre> Solution: <code>pip install build</code></p>"},{"location":"DEPLOY/#terraform-not-found","title":"Terraform not found","text":"<p><pre><code>[ERROR] Terraform not found. Install from: https://www.terraform.io/downloads\n</code></pre> Solution: Install Terraform or use <code>--skip-terraform</code></p>"},{"location":"DEPLOY/#aws-cli-not-found","title":"AWS CLI not found","text":"<p><pre><code>[ERROR] AWS CLI not found. Install: pip install awscli\n</code></pre> Solution: <code>pip install awscli</code> and configure credentials</p>"},{"location":"DEPLOY/#bucket-doesnt-exist","title":"Bucket doesn't exist","text":"<p><pre><code>[ERROR] Upload failed: The specified bucket does not exist\n</code></pre> Solution: Run without <code>--skip-terraform</code> to create infrastructure first</p>"},{"location":"DEPLOY/#no-wheel-file-found","title":"No wheel file found","text":"<p><pre><code>[ERROR] No wheel file found. Run without --skip-build first\n</code></pre> Solution: Remove <code>--skip-build</code> flag or run <code>python -m build</code> manually</p>"},{"location":"DEPLOY/#cost-considerations","title":"Cost Considerations","text":"<p>With <code>alur deploy</code>, you create: - S3 buckets: $0.023/GB stored (first 50 TB) - Glue jobs: $0.44/DPU-hour (only when running) - DynamoDB: On-demand pricing (only when accessed)</p> <p>Idle cost: $0.00 when not running jobs!</p>"},{"location":"DEPLOY/#advanced-multi-environment-setup","title":"Advanced: Multi-Environment Setup","text":"<pre><code># Deploy to multiple environments\nalur deploy --env dev\nalur deploy --env staging\nalur deploy --env prod --auto-approve\n\n# Use different AWS profiles\nAWS_PROFILE=dev-account alur deploy --env dev\nAWS_PROFILE=prod-account alur deploy --env prod\n</code></pre>"},{"location":"DEPLOY/#see-also","title":"See Also","text":"<ul> <li>AWS Deployment Guide - Detailed architecture</li> <li>Local Testing - Test before deploying</li> <li>Quick Start - Getting started guide</li> </ul>"},{"location":"SCHEDULING/","title":"Pipeline Scheduling","text":"<p>Alur supports automatic pipeline scheduling via AWS Glue SCHEDULED triggers. Use the <code>@schedule</code> decorator to configure cron-based execution for your pipelines.</p>"},{"location":"SCHEDULING/#table-of-contents","title":"Table of Contents","text":"<ul> <li>Quick Start</li> <li>Decorator API</li> <li>Cron Format Reference</li> <li>Common Patterns</li> <li>Deployment</li> <li>Managing Schedules</li> <li>Best Practices</li> <li>Limitations</li> </ul>"},{"location":"SCHEDULING/#quick-start","title":"Quick Start","text":"<p>Add scheduling to any pipeline using the <code>@schedule</code> decorator:</p> <pre><code>from alur import schedule, pipeline\nfrom contracts.bronze import OrdersBronze\n\n@schedule(\n    cron=\"0 2 * * ? *\",  # Daily at 2 AM UTC\n    description=\"Daily order ingestion\"\n)\n@pipeline(sources={}, target=OrdersBronze)\ndef ingest_orders():\n    \"\"\"Ingest orders from S3 daily.\"\"\"\n    return load_to_bronze(...)\n</code></pre> <p>Important: The <code>@schedule</code> decorator must be placed before the <code>@pipeline</code> decorator.</p>"},{"location":"SCHEDULING/#decorator-api","title":"Decorator API","text":""},{"location":"SCHEDULING/#schedulecron-enabledtrue-timezoneutc-descriptionnone-max_concurrent_runs1","title":"<code>@schedule(cron, enabled=True, timezone=\"UTC\", description=None, max_concurrent_runs=1)</code>","text":"<p>Configure automatic pipeline execution.</p> <p>Parameters:</p> <ul> <li><code>cron</code> (str, required): AWS Glue cron expression (6 fields)</li> <li><code>enabled</code> (bool, default=True): Whether schedule is active</li> <li><code>timezone</code> (str, default=\"UTC\"): Timezone for documentation (Glue triggers use UTC)</li> <li><code>description</code> (str, optional): Human-readable description</li> <li><code>max_concurrent_runs</code> (int, default=1): Maximum concurrent executions</li> </ul> <p>Example:</p> <pre><code>@schedule(\n    cron=\"0 */4 * * ? *\",\n    enabled=True,\n    description=\"Ingest orders every 4 hours\",\n    max_concurrent_runs=1\n)\n@pipeline(sources={}, target=OrdersBronze)\ndef ingest_orders():\n    pass\n</code></pre>"},{"location":"SCHEDULING/#cron-format-reference","title":"Cron Format Reference","text":"<p>AWS Glue SCHEDULED triggers use 6-field cron format, different from Unix cron (5 fields).</p>"},{"location":"SCHEDULING/#format","title":"Format","text":"<pre><code>cron(minute hour day-of-month month day-of-week year)\n</code></pre>"},{"location":"SCHEDULING/#field-values","title":"Field Values","text":"Field Values Wildcards minute 0-59 , - * / hour 0-23 , - * / day-of-month 1-31 , - * ? / month 1-12 or JAN-DEC , - * / day-of-week 1-7 or SUN-SAT , - * ? / year 1970-2199 , - * /"},{"location":"SCHEDULING/#important-rules","title":"Important Rules","text":"<ol> <li>Use <code>?</code> (question mark) in either <code>day-of-month</code> OR <code>day-of-week</code> (not both)</li> <li><code>?</code> means \"no specific value\"</li> <li> <p>Required to avoid ambiguity</p> </li> <li> <p>Wildcards:</p> </li> <li><code>*</code> - All values</li> <li><code>?</code> - No specific value (day fields only)</li> <li><code>-</code> - Range (e.g., <code>10-12</code>)</li> <li><code>,</code> - List (e.g., <code>MON,WED,FRI</code>)</li> <li><code>/</code> - Increments (e.g., <code>*/5</code> = every 5)</li> </ol>"},{"location":"SCHEDULING/#common-patterns","title":"Common Patterns","text":""},{"location":"SCHEDULING/#daily-schedules","title":"Daily Schedules","text":"<pre><code># Daily at 2 AM UTC\ncron=\"0 2 * * ? *\"\n\n# Daily at midnight UTC\ncron=\"0 0 * * ? *\"\n\n# Daily at 6 PM UTC\ncron=\"0 18 * * ? *\"\n</code></pre>"},{"location":"SCHEDULING/#hourly-schedules","title":"Hourly Schedules","text":"<pre><code># Every hour\ncron=\"0 * * * ? *\"\n\n# Every 4 hours\ncron=\"0 */4 * * ? *\"\n\n# Every 30 minutes\ncron=\"*/30 * * * ? *\"\n</code></pre>"},{"location":"SCHEDULING/#weekly-schedules","title":"Weekly Schedules","text":"<pre><code># Monday at 9 AM UTC\ncron=\"0 9 ? * MON *\"\n\n# Every weekday at 8 AM UTC\ncron=\"0 8 ? * MON-FRI *\"\n\n# Sunday at midnight UTC\ncron=\"0 0 ? * SUN *\"\n</code></pre>"},{"location":"SCHEDULING/#monthly-schedules","title":"Monthly Schedules","text":"<pre><code># First day of month at midnight UTC\ncron=\"0 0 1 * ? *\"\n\n# Last day of month (not directly supported, use Lambda)\n# Workaround: Schedule daily and check date in pipeline\n\n# 15th of every month at 3 AM UTC\ncron=\"0 3 15 * ? *\"\n</code></pre>"},{"location":"SCHEDULING/#custom-schedules","title":"Custom Schedules","text":"<pre><code># Twice daily (6 AM and 6 PM UTC)\ncron=\"0 6,18 * * ? *\"\n\n# Every 15 minutes during business hours (9 AM - 5 PM UTC)\ncron=\"*/15 9-17 * * ? *\"\n\n# First Monday of every month at 10 AM UTC\ncron=\"0 10 ? * MON#1 *\"\n</code></pre>"},{"location":"SCHEDULING/#deployment","title":"Deployment","text":"<p>Schedules are automatically deployed as AWS Glue SCHEDULED triggers during deployment.</p>"},{"location":"SCHEDULING/#deploy-with-schedules","title":"Deploy with Schedules","text":"<pre><code># Deploy infrastructure with schedules\nalur deploy --env production\n\n# Verify Glue SCHEDULED triggers created\naws glue list-triggers --max-results 20\n</code></pre>"},{"location":"SCHEDULING/#generated-infrastructure","title":"Generated Infrastructure","text":"<p>The <code>@schedule</code> decorator automatically generates:</p> <ol> <li>Glue SCHEDULED Trigger - Cron-based trigger that invokes the Glue job</li> </ol> <p>All Terraform code is auto-generated in <code>terraform/schedules.tf</code>.</p>"},{"location":"SCHEDULING/#managing-schedules","title":"Managing Schedules","text":""},{"location":"SCHEDULING/#list-schedules","title":"List Schedules","text":"<pre><code># List all schedules\nalur schedules\n\n# List only enabled schedules\nalur schedules --enabled-only\n</code></pre> <p>Example output:</p> <pre><code>================================================================================\nSCHEDULED PIPELINES\n================================================================================\n\nPipeline                       Schedule                  Status     Description\n--------------------------------------------------------------------------------\ningest_orders                  0 2 * * ? *               \u2713 Enabled  Daily order ingestion\ningest_customers               0 3 * * ? *               \u2713 Enabled  Daily customer sync\nprocess_analytics              0 */4 * * ? *             \u2713 Enabled  Hourly analytics update\n\nTotal: 3 schedule(s) | Enabled: 3 | Disabled: 0\n</code></pre>"},{"location":"SCHEDULING/#disable-a-schedule","title":"Disable a Schedule","text":"<p>To temporarily disable a schedule without removing the decorator:</p> <pre><code>@schedule(\n    cron=\"0 2 * * ? *\",\n    enabled=False,  # Disable schedule\n    description=\"Daily ingestion (currently disabled)\"\n)\n@pipeline(sources={}, target=OrdersBronze)\ndef ingest_orders():\n    pass\n</code></pre> <p>Redeploy to remove the Glue SCHEDULED trigger:</p> <pre><code>alur deploy --env production\n</code></pre>"},{"location":"SCHEDULING/#remove-a-schedule","title":"Remove a Schedule","text":"<p>Simply remove the <code>@schedule</code> decorator and redeploy:</p> <pre><code># Before\n@schedule(cron=\"0 2 * * ? *\")\n@pipeline(sources={}, target=OrdersBronze)\ndef ingest_orders():\n    pass\n\n# After\n@pipeline(sources={}, target=OrdersBronze)\ndef ingest_orders():\n    pass\n</code></pre> <pre><code>alur deploy --env production\n</code></pre>"},{"location":"SCHEDULING/#best-practices","title":"Best Practices","text":""},{"location":"SCHEDULING/#1-use-descriptive-descriptions","title":"1. Use Descriptive Descriptions","text":"<pre><code># Good\n@schedule(\n    cron=\"0 2 * * ? *\",\n    description=\"Daily ingestion of orders from production database\"\n)\n\n# Bad\n@schedule(cron=\"0 2 * * ? *\")  # No description\n</code></pre>"},{"location":"SCHEDULING/#2-prevent-concurrent-runs","title":"2. Prevent Concurrent Runs","text":"<p>For pipelines that shouldn't overlap:</p> <pre><code>@schedule(\n    cron=\"0 */4 * * ? *\",\n    max_concurrent_runs=1  # Prevent overlapping executions\n)\n@pipeline(sources={}, target=OrdersBronze)\ndef long_running_pipeline():\n    pass\n</code></pre>"},{"location":"SCHEDULING/#3-use-utc-for-consistency","title":"3. Use UTC for Consistency","text":"<p>Glue always uses UTC internally. Avoid timezone confusion by thinking in UTC:</p> <pre><code># Good - Explicit UTC time\n@schedule(\n    cron=\"0 14 * * ? *\",  # 2 PM UTC\n    description=\"Daily ingestion at 2 PM UTC (9 AM EST)\"\n)\n\n# Confusing - Timezone parameter is for documentation only\n@schedule(\n    cron=\"0 14 * * ? *\",\n    timezone=\"America/New_York\",  # Misleading - Glue still uses UTC!\n    description=\"Daily ingestion at 9 AM EST\"\n)\n</code></pre>"},{"location":"SCHEDULING/#4-test-schedules-manually-first","title":"4. Test Schedules Manually First","text":"<p>Before scheduling, test the pipeline manually:</p> <pre><code># Test pipeline on Glue\nalur run ingest_orders --env dev\n\n# Check logs\nalur logs --tail --env dev\n\n# If successful, deploy schedule\nalur deploy --env production\n</code></pre>"},{"location":"SCHEDULING/#5-monitor-scheduled-runs","title":"5. Monitor Scheduled Runs","text":"<p>Check recent executions:</p> <pre><code># View Glue job runs\naws glue get-job-runs --job-name alur-ingest_orders-production --max-results 10\n\n# View Glue SCHEDULED trigger details\naws glue get-trigger --name alur-ingest_orders-schedule-production\n</code></pre>"},{"location":"SCHEDULING/#6-use-idempotency","title":"6. Use Idempotency","text":"<p>Ensure your pipelines are idempotent (safe to run multiple times):</p> <pre><code>@schedule(cron=\"0 2 * * ? *\")\n@pipeline(sources={}, target=OrdersBronze)\ndef ingest_orders():\n    \"\"\"\n    Idempotent ingestion - files tracked in DynamoDB.\n    Re-running won't create duplicates.\n    \"\"\"\n    return load_to_bronze(\n        spark,\n        source_path=\"s3://bucket/orders/*.csv\",\n        target=OrdersBronze,\n        enable_idempotency=True  # Prevents duplicate ingestion\n    )\n</code></pre>"},{"location":"SCHEDULING/#limitations","title":"Limitations","text":""},{"location":"SCHEDULING/#1-one-schedule-per-pipeline","title":"1. One Schedule Per Pipeline","text":"<p>Each pipeline can only have one schedule:</p> <pre><code># \u2717 NOT ALLOWED\n@schedule(cron=\"0 2 * * ? *\")\n@schedule(cron=\"0 14 * * ? *\")  # Error: already registered\n@pipeline(sources={}, target=OrdersBronze)\ndef ingest_orders():\n    pass\n\n# \u2713 WORKAROUND: Create separate pipelines\n@schedule(cron=\"0 2 * * ? *\")\n@pipeline(sources={}, target=OrdersBronze)\ndef ingest_orders_morning():\n    return _shared_ingestion_logic()\n\n@schedule(cron=\"0 14 * * ? *\")\n@pipeline(sources={}, target=OrdersBronze)\ndef ingest_orders_afternoon():\n    return _shared_ingestion_logic()\n</code></pre>"},{"location":"SCHEDULING/#2-glue-uses-utc-only","title":"2. Glue Uses UTC Only","text":"<p>All schedules run in UTC. Glue does not support timezone-aware scheduling.</p>"},{"location":"SCHEDULING/#3-minimum-frequency-1-minute","title":"3. Minimum Frequency: 1 Minute","text":"<p>Glue has a minimum interval of 1 minute. For sub-minute scheduling, use alternative solutions (Lambda, Kinesis, etc.).</p>"},{"location":"SCHEDULING/#4-execution-not-guaranteed","title":"4. Execution Not Guaranteed","text":"<p>Glue is \"at least once\" delivery. In rare cases, a schedule may trigger multiple times or not at all. Design pipelines to handle this.</p>"},{"location":"SCHEDULING/#troubleshooting","title":"Troubleshooting","text":""},{"location":"SCHEDULING/#schedule-not-triggering","title":"Schedule Not Triggering","text":"<ol> <li> <p>Check Glue SCHEDULED trigger status: <pre><code>aws glue get-trigger --name alur-ingest_orders-schedule-production\n</code></pre></p> </li> <li> <p>Verify trigger is ACTIVATED: <pre><code>{\n  \"State\": \"ACTIVATED\",  // Should be ACTIVATED\n  \"Type\": \"SCHEDULED\",\n  \"Schedule\": \"cron(0 2 * * ? *)\"\n}\n</code></pre></p> </li> <li> <p>Glue triggers don't require additional IAM permissions - the Glue job role is used automatically</p> </li> <li> <p>Verify Glue job exists: <pre><code>aws glue get-job --job-name alur-ingest_orders-production\n</code></pre></p> </li> </ol>"},{"location":"SCHEDULING/#invalid-cron-expression","title":"Invalid Cron Expression","text":"<p>Error: <code>ValueError: Glue cron expression must have 6 fields</code></p> <p>Solution: Use Glue format (6 fields), not Unix cron (5 fields):</p> <pre><code># \u2717 Unix cron (5 fields)\n@schedule(cron=\"0 2 * * *\")  # ERROR\n\n# \u2713 Glue cron (6 fields)\n@schedule(cron=\"0 2 * * ? *\")  # CORRECT\n</code></pre>"},{"location":"SCHEDULING/#duplicate-schedule-error","title":"Duplicate Schedule Error","text":"<p>Error: <code>ValueError: Schedule for pipeline 'ingest_orders' is already registered</code></p> <p>Solution: Remove duplicate <code>@schedule</code> decorator. Each pipeline can only have one schedule.</p>"},{"location":"SCHEDULING/#cost-considerations","title":"Cost Considerations","text":"<ul> <li>Glue Rules: Free for first 100 rules/month</li> <li>Glue Invocations: $1.00 per million invocations</li> <li>Glue Job Runs: Standard Glue pricing (based on DPU-hours)</li> </ul> <p>Example Cost (Daily Schedule): - 1 pipeline \u00d7 1 run/day \u00d7 30 days = 30 invocations/month - Glue cost: 30 / 1,000,000 \u00d7 $1.00 = $0.00003/month (negligible) - Glue cost: Depends on job runtime (unchanged from manual runs)</p> <p>Conclusion: Scheduling adds virtually zero cost overhead. You only pay for Glue execution time.</p>"}]}